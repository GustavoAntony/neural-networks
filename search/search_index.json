{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neural Networks &amp; Deep Learning - Gustavo Antony","text":"<p>This site presents practical exercises and documentation related to artificial neural networks and deep learning. It includes data generation, preprocessing, model training, and evaluation.</p> <p>Use the navigation menu to access exercises, view plots, and follow the data processing and model development steps.</p>"},{"location":"pages/data/exercise1/","title":"Exploring Class Separability in 2D","text":""},{"location":"pages/data/exercise1/#1-generate-the-data","title":"1. Generate the Data","text":"<pre><code>params = {\n        0: {MEAN: [2, 3], STD: [0.8,2.5]},\n        1: {MEAN: [5, 6], STD: [1.2,1.9]},\n        2: {MEAN: [8, 1], STD: [0.9,0.9]},\n        3: {MEAN: [15, 4], STD: [0.5,2]},\n        }\n\nX = []\ny = []\n\nfor cls, p in params.items():\n    points = np.random.normal(loc=p[\"mean\"], scale=p[\"std\"], size=(100, 2))\n    X.append(points)\n    y.append(np.full(100, cls))\n\nX = np.vstack(X)\ny = np.hstack(y)\n</code></pre> <p>We're generating a synthetic dataset to simulate a classification problem. The data consists of four distinct classes, each following a bivariate normal distribution. This is a common practice for creating simple, interpretable datasets to test machine learning algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Neural Networks.</p> <p>We'll use a dictionary to define the parameters for each class: its mean (center) and standard deviation (spread). For each class, we generate 100 data points. The X array stores the 2D coordinates of these points, and the y array stores their corresponding class labels.</p>"},{"location":"pages/data/exercise1/#key-concepts","title":"Key Concepts:","text":"<ul> <li> <p>Bivariate Normal Distribution: A probability distribution that describes two correlated variables. In our case, the two variables are the x and y coordinates of our data points. The mean parameter defines the center of the cluster, and the std parameter defines how spread out the data points are along each dimension.</p> </li> <li> <p>np.random.normal(): This NumPy function is used to generate random numbers from a normal (Gaussian) distribution. We use it to create our data points.</p> </li> <li> <p>np.vstack() and np.hstack(): These functions combine the generated points and labels from each class into single, continuous NumPy arrays.</p> </li> </ul>"},{"location":"pages/data/exercise1/#2-plot-the-data","title":"2. Plot the data","text":"<pre><code>colors = ['red', 'blue', 'green', 'purple']\n\nplt.figure(figsize=(8, 6))\nfor cls in params.keys():\n    plt.scatter(X[y == cls, 0], X[y == cls, 1], c=colors[cls], label=f\"Class {cls}\", alpha=0.6)\n\nplt.xlabel(\"Label 1\")\nplt.ylabel(\"Label 2\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'data','exercise1_2.png'))\n</code></pre> <p>After generating the data, a scatter plot is the ideal way to visualize the distribution of each class. This visual representation helps us understand the data's characteristics, identify patterns, and gauge how separable the classes are.</p>"},{"location":"pages/data/exercise1/#3-analyze-and-draw-boundaries","title":"3. Analyze and Draw Boundaries:","text":"<pre><code>colors = ['red', 'blue', 'green', 'purple']\n\nplt.figure(figsize=(8, 6))\nfor cls in params.keys():\n    plt.scatter(X[y == cls, 0], X[y == cls, 1], c=colors[cls], label=f\"Class {cls}\", alpha=0.6)\n\nplt.xlabel(\"Label 1\")\nplt.ylabel(\"Label 2\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.plot([2.5, 11.5], [-3, 9], color='black', linestyle='--', linewidth=2, label=\"Boundary 0-1\")\nplt.plot([2.0, 4.5], [12.5, -0.2], color='black', linestyle='--', linewidth=2, label=\"Boundary 1-2\")\nplt.plot([11.5, 11.5], [-2, 14], color='black', linestyle='--', linewidth=2, label=\"Boundary 2-3\")\n\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'data','exercise1_3.png'))\n</code></pre> <p>With the data visualized, we can analyze the class separation. Our goal is to determine if a simple linear model could effectively classify the data or if a more complex model is required. We can visually represent potential decision boundaries to test this hypothesis.</p> <p>The scatter plot shows that the clusters have some overlap, especially between classes 0, 1, and 2. Because of this overlap, a single straight line cannot perfectly separate all four classes. This is a classic example of a non-linearly separable dataset.</p> <p>However, a combination of multiple linear boundaries can effectively separate the regions. This is what multi-layer neural networks do, they use a series of linear transformations to create complex, non-linear decision boundaries. The added lines in the plot below demonstrate that a series of linear splits can achieve near-perfect separation.</p>"},{"location":"pages/data/exercise1/#final-code-integrated-solution","title":"Final Code: Integrated Solution","text":"<p>Here is the complete code for generating the data, visualizing it, and analyzing the boundaries:</p> <pre><code>def exercise1():\n    params = {\n        0: {MEAN: [2, 3], STD: [0.8,2.5]},\n        1: {MEAN: [5, 6], STD: [1.2,1.9]},\n        2: {MEAN: [8, 1], STD: [0.9,0.9]},\n        3: {MEAN: [15, 4], STD: [0.5,2]},\n        }\n\n    X = []\n    y = []\n\n    for cls, p in params.items():\n        points = np.random.normal(loc=p[\"mean\"], scale=p[\"std\"], size=(100, 2))\n        X.append(points)\n        y.append(np.full(100, cls))\n\n    X = np.vstack(X)\n    y = np.hstack(y)\n\n    colors = ['red', 'blue', 'green', 'purple']\n\n    plt.figure(figsize=(8, 6))\n    for cls in params.keys():\n        plt.scatter(X[y == cls, 0], X[y == cls, 1], c=colors[cls], label=f\"Class {cls}\", alpha=0.6)\n\n    plt.xlabel(\"Label 1\")\n    plt.ylabel(\"Label 2\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'data','exercise1_2.png'))\n\n    plt.plot([2.5, 11.5], [-3, 9], color='black', linestyle='--', linewidth=2, label=\"Boundary 0-1\")\n    plt.plot([2.0, 4.5], [12.5, -0.2], color='black', linestyle='--', linewidth=2, label=\"Boundary 1-2\")\n    plt.plot([11.5, 11.5], [-2, 14], color='black', linestyle='--', linewidth=2, label=\"Boundary 2-3\")\n\n    plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'data','exercise1_3.png'))\n</code></pre>"},{"location":"pages/data/exercise2/","title":"Non-Linearity in Higher Dimensions","text":""},{"location":"pages/data/exercise2/#generate-the-data","title":"Generate the Data","text":"<pre><code>params = {\n    'A' : {MEAN : [0,0,0,0,0], COV : [[1,0.8,0.1,0,0],[0.8,1,0.3,0,0],[0.1,0.3,1,0.5,0],[0,0,0.5,1,0.2],[0,0,0,0.2,1.0]]},\n    'B' : {MEAN : [1.5,1.5,1.5,1.5,1.5], COV : [[1.5,-0.7,0.2,0,0],[-0.7,1.5,0.4,0,0],[0.2,0.4,1.5,0.6,0],[0,0,0.6,1.5,0.3],[0,0,0,0.3,1.5]]}\n}\n\ncls_A = np.random.multivariate_normal(params['A'][MEAN], params['A'][COV], 500)\ncls_B = np.random.multivariate_normal(params['B'][MEAN], params['B'][COV], 500)\n</code></pre> <p>We're creating a synthetic dataset with two distinct classes, A and B, in a 5-dimensional space. This is a common practice for creating complex, non-linear datasets that can't be easily visualized. Each class is defined by a multivariate normal distribution, a powerful way to model data clusters by specifying a central point (the mean vector) and the spread and relationship between its dimensions (the covariance matrix).</p> <p>The covariance matrix is particularly important here. It's a 5x5 matrix that describes how each feature in the 5D space relates to the others. The diagonal elements represent the variance of each feature, while the off-diagonal elements show the covariance between them. A high positive or negative covariance indicates that two features are strongly related. For example, in class B, the negative covariance of -0.7 between the first two features indicates that as one increases, the other tends to decrease.</p>"},{"location":"pages/data/exercise2/#plot-the-data","title":"Plot the Data","text":"<pre><code>labels_A = np.zeros(500)\nlabels_B = np.ones(500)\n\nX = np.vstack((cls_A, cls_B))\ny = np.hstack((labels_A, labels_B))\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n</code></pre> <p>Since our dataset exists in a 5D space, direct visualization is impossible. To analyze the data's structure, we use Principal Component Analysis (PCA) to reduce its dimensionality to 2D.</p> <p>PCA works by identifying the directions (called principal components) that capture the most variance in the data. By projecting the high-dimensional data onto the first two principal components, we create a 2D representation that preserves as much of the original information as possible. This allows us to visualize the clusters and their relationships.</p> <pre><code>plt.figure(figsize=(8,6))\nplt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], label='Classe A', alpha=0.7)\nplt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], label='Classe B', alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'data','exercise2.png'))\nplt.close()\n</code></pre> <p></p> <p>The scatter plot of the PCA-reduced data reveals how the two classes are distributed in the lower-dimensional space. The plot shows that classes A and B have significant overlap. This is a key insight.</p> <p>The overlap indicates that the data is not linearly separable. A simple linear classifier, like Logistic Regression or a single-layer perceptron, would struggle to find a single straight line to accurately separate the two classes. This type of non-linear problem is a challenge for simple models but is a perfect use case for more complex, non-linear models like Multi-Layer Perceptrons (Neural Networks), which can learn intricate decision boundaries.</p>"},{"location":"pages/data/exercise2/#final-code-integrated-solution","title":"Final Code: Integrated Solution","text":"<p>Here is the complete code for generating the data, applying PCA, and visualizing the results:</p> <pre><code>def exercise2():\n    params = {\n        'A' : {MEAN : [0,0,0,0,0], COV : [[1,0.8,0.1,0,0],[0.8,1,0.3,0,0],[0.1,0.3,1,0.5,0],[0,0,0.5,1,0.2],[0,0,0,0.2,1.0]]},\n        'B' : {MEAN : [1.5,1.5,1.5,1.5,1.5], COV : [[1.5,-0.7,0.2,0,0],[-0.7,1.5,0.4,0,0],[0.2,0.4,1.5,0.6,0],[0,0,0.6,1.5,0.3],[0,0,0,0.3,1.5]]}\n    }\n\n    cls_A = np.random.multivariate_normal(params['A'][MEAN], params['A'][COV], 500)\n    cls_B = np.random.multivariate_normal(params['B'][MEAN], params['B'][COV], 500)\n\n    labels_A = np.zeros(500)\n    labels_B = np.ones(500)\n\n    X = np.vstack((cls_A, cls_B))\n    y = np.hstack((labels_A, labels_B))\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    plt.figure(figsize=(8,6))\n    plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], label='Classe A', alpha=0.7)\n    plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], label='Classe B', alpha=0.7)\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n    plt.legend()\n    plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'data','exercise2.png'))\n    plt.close()\n</code></pre>"},{"location":"pages/data/exercise3/","title":"Preparing Real-World Data for a Neural Network","text":""},{"location":"pages/data/exercise3/#get-the-data","title":"Get the Data","text":"<p>Data downloaded from : Spaceship Titanic (https://www.kaggle.com/competitions/spaceship-titanic/data)</p> <p></p>"},{"location":"pages/data/exercise3/#describe-the-data","title":"Describe the Data","text":"<p>This documentation provides a comprehensive overview of the data preprocessing steps for a machine learning task, using the Spaceship Titanic dataset as a case study. The goal of preprocessing is to transform raw data into a clean, well-structured format that can be effectively used by machine learning models.</p> <p>The objective of this training dataset is to predict whether a passenger was transported (\"Transported\") or not during a space voyage, based on various personal and consumption characteristics. </p> <p>We've categorized the features into three groups for clarity: numerical, categorical, and special columns.</p> <pre><code>numerical_features = ['Age', 'RoomService','FoodCourt','ShoppingMall', 'Spa', 'VRDeck']\ncategorical_features = ['HomePlanet','CryoSleep','Destination','VIP']\nspecial_columns = ['Name', 'Cabin', 'PassengerId']\ntarget_column = 'Transported'\n</code></pre> <p><pre><code>df.info()\n</code></pre> <pre><code>RangeIndex: 8693 entries, 0 to 8692\nData columns (total 14 columns):\n #   Column        Non-Null Count  Dtype\n---  ------        --------------  -----\n 0   PassengerId   8693 non-null   object\n 1   HomePlanet    8492 non-null   object\n 2   CryoSleep     8476 non-null   object\n 3   Cabin         8494 non-null   object\n 4   Destination   8511 non-null   object\n 5   Age           8514 non-null   float64\n 6   VIP           8490 non-null   object\n 7   RoomService   8512 non-null   float64\n 8   FoodCourt     8510 non-null   float64\n 9   ShoppingMall  8485 non-null   float64\n 10  Spa           8510 non-null   float64\n 11  VRDeck        8505 non-null   float64\n 12  Name          8493 non-null   object\n 13  Transported   8693 non-null   bool\ndtypes: bool(1), float64(6), object(7)\nmemory usage: 891.5+ KB\n</code></pre> <pre><code>print(df.isnull().sum())\n</code></pre> <pre><code>PassengerId       0\nHomePlanet      201\nCryoSleep       217\nCabin           199\nDestination     182\nAge             179\nVIP             203\nRoomService     181\nFoodCourt       183\nShoppingMall    208\nSpa             183\nVRDeck          188\nName            200\nTransported       0\ndtype: int64\n</code></pre></p> <p>The df.info() and df.isnull().sum() outputs provide a critical initial look at the data, revealing column data types and the presence of missing values in nearly all features. This highlights the need for a robust preprocessing pipeline.</p>"},{"location":"pages/data/exercise3/#preprocess-the-data","title":"Preprocess the Data","text":"<p>Handling Special Columns</p> <ul> <li> <p>Name and PassengerId: These are unique identifiers that have no predictive power for the model. We drop them to prevent the model from memorizing individual records, which would lead to overfitting.</p> </li> <li> <p>Cabin: The cabin feature is structured as Deck/Num/Side. The Deck and Side (P for Port, S for Starboard) likely hold valuable information about the passenger's location, which could influence their outcome. The Num is a unique identifier, similar to a name or ID, and is not useful for prediction, so we can ignore it. We use a regular expression to extract the Deck and Side into two new categorical columns before dropping the original Cabin column.</p> </li> </ul> <pre><code>df.drop(columns=['Name', 'PassengerId'], inplace=True)\ndf[['Cabin_Deck', 'Cabin_Side']] = df['Cabin'].str.extract(r'([A-Z]+)\\/\\d+\\/([A-Z]+)')\ndf.drop(columns=['Cabin'], inplace=True)\n\ncategorical_features.append('Cabin_Side')\ncategorical_features.append('Cabin_Deck')\n</code></pre> <p>Imputing Missing Values</p> <ul> <li> <p>Numerical Features: Missing values in numerical columns (Age, RoomService, etc.) are filled with the median. The median is used instead of the mean because it is less sensitive to outliers, ensuring that a few extreme values do not distort the central tendency of the data.</p> </li> <li> <p>Categorical Features: Missing values in categorical columns are filled with the mode (the most frequent value). This is a simple and effective strategy that preserves the original distribution of each category.</p> </li> </ul> <pre><code>for col in numerical_features:\n        df[col] = df[col].fillna(df[col].median())\nfor col in categorical_features:\n    df[col] = df[col].fillna(df[col].mode()[0])\n</code></pre> <p>Feature Engineering and Scaling</p> <ul> <li> <p>Encoding Categorical Features: Machine learning algorithms require numerical input. We use One-Hot Encoding to convert categorical features (HomePlanet, Cabin_Deck, etc.) into a binary format. This creates a new column for each category, with a value of 1 for the relevant category and 0 otherwise. The drop_first=True argument prevents multicollinearity by dropping one of the generated columns.</p> </li> <li> <p>Scaling Numerical Features: Features with different ranges can bias a model. We use Min-Max Scaling to transform all numerical features to a common range of [-1, 1]. This ensures that each feature contributes equally to the model, preventing features with large values from dominating the learning process.</p> </li> </ul> <pre><code>df_encoded = pd.get_dummies(df[categorical_features], drop_first=True)\n</code></pre> <pre><code>scaler = MinMaxScaler(feature_range=(-1, 1))\ndf_scaled = pd.DataFrame(scaler.fit_transform(df[numerical_features]), columns=numerical_features)\n</code></pre> <p>Final Data Assembly</p> <p>The final step is to combine the scaled numerical features, the one-hot encoded categorical features, and the target variable (Transported) into a single processed DataFrame ready for model training.</p> <pre><code>df_processed = pd.concat([df_scaled, df_encoded], axis=1)\ndf_processed[target_column] = df[target_column].astype(int) \n</code></pre> <p></p>"},{"location":"pages/data/exercise3/#visualize-the-results","title":"Visualize the Results","text":"<p>Histograms are used to visualize the distribution of numerical features before and after scaling. The plots clearly demonstrate the effect of Min-Max Scaling, which normalizes the data to the [-1, 1] range, aligning all features on a consistent scale. This visual confirmation is crucial for validating that the preprocessing steps were performed correctly.</p> <p>The histograms show how the distributions of features like Age and RoomService are transformed from their original ranges to the [-1, 1] range. This normalization helps to ensure that each feature contributes equally to the model training process, improving convergence speed and overall model performance.</p> <pre><code>fig, axes = plt.subplots(2, len(numerical_features), figsize=(20, 8))\nfor i, col in enumerate(numerical_features):\n    df[col].plot.hist(bins=30, ax=axes[0, i], alpha=0.7, color='skyblue')\n    axes[0, i].set_title(f\"{col} (Before Scaling)\")\n    df_scaled[col].plot.hist(bins=30, ax=axes[1, i], alpha=0.7, color='salmon')\n    axes[1, i].set_title(f\"{col} (After Scaling)\")\n\nplt.tight_layout()\nplt.suptitle(\"Histograms of Numerical Features (Before and After Scaling)\", y=1.02)\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'data','exercise3_4.png'))\nplt.close()\n</code></pre> <p></p>"},{"location":"pages/data/exercise3/#final-code-integrated-solution","title":"Final Code: Integrated Solution","text":"<p>Here is the complete code for loading, preprocessing, and visualizing the dataset:</p> <pre><code>def exercise3():\n    df = pd.read_csv(os.path.join(DATA_OUTPUTS_FILE_PATH,'data','spaceship_titanic','train.csv'))\n    df_head = df.head(5)\n\n    numerical_features = ['Age', 'RoomService','FoodCourt','ShoppingMall', 'Spa', 'VRDeck']\n    categorical_features = ['HomePlanet','CryoSleep','Destination','VIP']\n    special_columns = ['Name', 'Cabin', 'PassengerId']\n    target_column = 'Transported'\n\n    df.info()\n    print(df.isnull().sum())\n\n    df.drop(columns=['Name', 'PassengerId'], inplace=True)\n    df[['Cabin_Deck', 'Cabin_Side']] = df['Cabin'].str.extract(r'([A-Z]+)\\/\\d+\\/([A-Z]+)')\n    df.drop(columns=['Cabin'], inplace=True)\n\n    categorical_features.append('Cabin_Side')\n    categorical_features.append('Cabin_Deck')\n\n    for col in numerical_features:\n        df[col] = df[col].fillna(df[col].median())\n    for col in categorical_features:\n        df[col] = df[col].fillna(df[col].mode()[0])\n\n    df_encoded = pd.get_dummies(df[categorical_features], drop_first=True)\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    df_scaled = pd.DataFrame(scaler.fit_transform(df[numerical_features]), columns=numerical_features)\n\n    df_processed = pd.concat([df_scaled, df_encoded], axis=1)\n    df_processed[target_column] = df[target_column].astype(int) \n\n    df_processed_head = df_processed.head(5)\n\n    fig, axes = plt.subplots(2, len(numerical_features), figsize=(20, 8))\n    for i, col in enumerate(numerical_features):\n        df[col].plot.hist(bins=30, ax=axes[0, i], alpha=0.7, color='skyblue')\n        axes[0, i].set_title(f\"{col} (Before Scaling)\")\n        df_scaled[col].plot.hist(bins=30, ax=axes[1, i], alpha=0.7, color='salmon')\n        axes[1, i].set_title(f\"{col} (After Scaling)\")\n\n    plt.tight_layout()\n    plt.suptitle(\"Histograms of Numerical Features (Before and After Scaling)\", y=1.02)\n    plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'data','exercise3_4.png'))\n    plt.close()\n</code></pre>"},{"location":"pages/mlp/exercise1/","title":"Manual Calculation of MLP Steps","text":"<p>This page details the step-by-step manual calculation of a single forward and backward pass for a simple Multi-Layer Perceptron (MLP). The network consists of two input features, one hidden layer with two neurons, and one output neuron.</p>"},{"location":"pages/mlp/exercise1/#network-and-data-specifications","title":"Network and Data Specifications","text":"<ul> <li>Input Vector (\\(x\\)): \\([0.5, -0.2]\\)</li> <li>True Output (\\(y\\)): \\(1.0\\)</li> <li>Hidden Layer Weights (\\(W^{(1)}\\)): \\(\\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix}\\)</li> <li>Hidden Layer Biases (\\(b^{(1)}\\)): \\([0.1, -0.2]\\)</li> <li>Output Layer Weights (\\(W^{(2)}\\)): \\([0.5, -0.3]\\)</li> <li>Output Layer Bias (\\(b^{(2)}\\)): \\(0.2\\)</li> <li>Activation Function: Hyperbolic tangent (tanh) for both layers.</li> <li>Loss Function: Mean Squared Error (MSE), \\(L = \\frac{1}{N}(y - \\hat{y})^2\\).</li> <li>Learning Rate (\\(\\eta\\)): \\(0.3\\)</li> </ul>"},{"location":"pages/mlp/exercise1/#1-forward-pass","title":"1. Forward Pass","text":"<p>The forward pass is the process of moving the input data through the network layers to produce an output.</p>"},{"location":"pages/mlp/exercise1/#hidden-layer-pre-activation-z1","title":"Hidden Layer Pre-activation (\\(z^{(1)}\\))","text":"\\[ z^{(1)} = \\begin{bmatrix} (0.3)(0.5) + (-0.1)(-0.2) \\\\ (0.2)(0.5) + (0.4)(-0.2) \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} = \\begin{bmatrix} 0.15 + 0.02 \\\\ 0.10 - 0.08 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} = \\begin{bmatrix} 0.17 \\\\ 0.02 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} = \\begin{bmatrix} 0.27 \\\\ -0.18 \\end{bmatrix} \\]"},{"location":"pages/mlp/exercise1/#hidden-layer-activations-a1","title":"Hidden Layer Activations (\\(a^{(1)}\\))","text":"\\[ a^{(1)} = \\tanh\\big(z^{(1)}\\big) = \\begin{bmatrix} \\tanh(0.27) \\\\ \\tanh(-0.18) \\end{bmatrix} \\approx \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix} \\]"},{"location":"pages/mlp/exercise1/#output-layer-pre-activation-z2","title":"Output Layer Pre-activation (\\(z^{(2)}\\))","text":"\\[ z^{(2)} = \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix} \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix} + 0.2 = (0.5)(0.2636) + (-0.3)(-0.1781) + 0.2 = 0.1318 + 0.0534 + 0.2 \\approx 0.3852 \\]"},{"location":"pages/mlp/exercise1/#final-output-haty","title":"Final Output (\\(\\hat{y}\\))","text":"\\[ \\hat{y} = \\tanh(z^{(2)}) = \\tanh(0.3852) \\approx 0.3672 \\]"},{"location":"pages/mlp/exercise1/#2-loss-calculation","title":"2. Loss Calculation","text":"\\[ L = (y - \\hat{y})^2 = (1.0 - 0.3672)^2 = (0.6328)^2 \\approx 0.4004 \\]"},{"location":"pages/mlp/exercise1/#3-backward-pass-backpropagation","title":"3. Backward Pass (Backpropagation)","text":""},{"location":"pages/mlp/exercise1/#gradient-of-loss-wrt-output-pre-activation","title":"Gradient of Loss w.r.t. Output Pre-activation","text":"\\[ \\frac{\\partial L}{\\partial z^{(2)}} = [-2(1 - \\hat{y})]\\cdot(1 - \\hat{y}^2) = [-2(0.6328)]\\cdot(1 - 0.1348) = -1.2656 \\cdot 0.8652 \\approx -1.0948 \\]"},{"location":"pages/mlp/exercise1/#gradients-for-the-output-layer-w2-b2","title":"Gradients for the Output Layer \\((W^{(2)}, b^{(2)})\\)","text":"\\[ \\frac{\\partial L}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial z^{(2)}} \\, a^{(1)T} = (-1.0948)\\cdot\\begin{bmatrix}0.2636 &amp; -0.1781\\end{bmatrix} \\approx \\begin{bmatrix}-0.2886 &amp; 0.1950\\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial b^{(2)}} = -1.0948 \\]"},{"location":"pages/mlp/exercise1/#propagating-to-the-hidden-layer","title":"Propagating to the Hidden Layer","text":"\\[ \\frac{\\partial L}{\\partial a^{(1)}} = W^{(2)T}\\cdot\\frac{\\partial L}{\\partial z^{(2)}} = \\begin{bmatrix}0.5\\\\ -0.3\\end{bmatrix}\\cdot(-1.0948) \\approx \\begin{bmatrix}-0.5474 \\\\ 0.3284\\end{bmatrix} \\] \\[ 1 - \\tanh^2(z^{(1)}) \\approx \\begin{bmatrix}1 - (0.2636)^2 \\\\ 1 - (-0.1781)^2\\end{bmatrix} \\approx \\begin{bmatrix}0.9306 \\\\ 0.9683\\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\begin{bmatrix}-0.5474 \\\\ 0.3284\\end{bmatrix} \\odot \\begin{bmatrix}0.9306 \\\\ 0.9683\\end{bmatrix} \\approx \\begin{bmatrix}-0.5094 \\\\ 0.3180\\end{bmatrix} \\]"},{"location":"pages/mlp/exercise1/#gradients-for-the-hidden-layer-w1-b1","title":"Gradients for the Hidden Layer \\((W^{(1)}, b^{(1)})\\)","text":"\\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} x^T = \\begin{bmatrix}-0.5094 \\\\ 0.3180\\end{bmatrix} \\begin{bmatrix}0.5 &amp; -0.2\\end{bmatrix} \\approx \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial b^{(1)}} = \\begin{bmatrix}-0.5094 \\\\ 0.3180\\end{bmatrix} \\]"},{"location":"pages/mlp/exercise1/#4-parameter-update","title":"4. Parameter Update","text":""},{"location":"pages/mlp/exercise1/#output-layer","title":"Output Layer","text":"<ul> <li>Weights</li> </ul> \\[ W^{(2)}_{new} = \\begin{bmatrix}0.5 &amp; -0.3\\end{bmatrix} - 0.3\\cdot\\begin{bmatrix}-0.2886 &amp; 0.1950\\end{bmatrix} = \\begin{bmatrix}0.5 &amp; -0.3\\end{bmatrix} - \\begin{bmatrix}-0.0866 &amp; 0.0585\\end{bmatrix} \\approx \\begin{bmatrix}0.5866 &amp; -0.3585\\end{bmatrix} \\] <ul> <li>Bias</li> </ul> \\[b^{(2)}_{new} = 0.2 - 0.3\\cdot(-1.0948) = 0.2 + 0.3284 \\approx 0.5284\\]"},{"location":"pages/mlp/exercise1/#hidden-layer","title":"Hidden Layer","text":"<ul> <li>Weights</li> </ul> \\[ W^{(1)}_{new} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} - 0.3\\cdot \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} - \\begin{bmatrix} -0.0764 &amp; 0.0306 \\\\ 0.0477 &amp; -0.0191 \\end{bmatrix} \\approx \\begin{bmatrix} 0.3764 &amp; -0.1306 \\\\ 0.1523 &amp; 0.4191 \\end{bmatrix} \\] <ul> <li>Biases</li> </ul> \\[ b^{(1)}_{new} = \\begin{bmatrix}0.1 \\\\ -0.2\\end{bmatrix} - 0.3\\cdot \\begin{bmatrix}-0.5094 \\\\ 0.3180\\end{bmatrix} = \\begin{bmatrix}0.1 \\\\ -0.2\\end{bmatrix} - \\begin{bmatrix}-0.1528 \\\\ 0.0954\\end{bmatrix} \\approx \\begin{bmatrix}0.2528 \\\\ -0.2954\\end{bmatrix} \\] <p>*The creation of this page was assisted by Google Gemini.</p>"},{"location":"pages/mlp/exercise2/","title":"Binary Classification with Synthetic Data and Scratch MLP","text":"<p>This exercise demonstrates how to build and train a simple Multi-Layer Perceptron (MLP) for a binary classification problem using synthetic data. The process includes data generation, visualization, model setup, training, evaluation, and visualization of results.</p>"},{"location":"pages/mlp/exercise2/#data-generation","title":"Data Generation","text":"<p>We generate two classes of synthetic data using <code>make_classification</code>, with some noise and class separation. The data is split into training and test sets.</p> <pre><code>X_train, X_test, y_train, y_test = generate_binary_data()\n</code></pre>"},{"location":"pages/mlp/exercise2/#data-visualization","title":"Data Visualization","text":"<p>Before training, it's important to visualize the data distribution. Here, we plot the training data to see the class separation in 2D.</p> <pre><code>plt.scatter(X_train[:,0], X_train[:,1], c=y_train.flatten(), cmap='bwr', alpha=0.6)\nplt.title('Training Data - Exercise 2')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.grid(True, alpha=0.3)\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'mlp','exercise2_data.png'))\nplt.close()\n</code></pre> <p></p>"},{"location":"pages/mlp/exercise2/#model-initialization","title":"Model Initialization","text":"<p>We define the MLP architecture: input size, one hidden layer, output size, and learning rate. The model is created for binary classification.</p> <pre><code>input_size = X_train.shape[1]\nhidden_sizes = [16]\noutput_size = 1\nlearning_rate = 0.05\nmlp = MLP(input_size, hidden_sizes, output_size, learning_rate, task='binary')\n</code></pre>"},{"location":"pages/mlp/exercise2/#training-and-accuracy-per-epoch","title":"Training and Accuracy per Epoch","text":"<p>The model is trained for 2000 epochs. After each epoch, we compute and store the training accuracy to monitor learning progress.</p> <pre><code>train_accuracies = []\nepochs = 2000\nfor epoch in range(epochs):\n    mlp.train(X_train, y_train, 1)\n    y_train_pred = mlp.forward(X_train)\n    y_train_pred_classes = (y_train_pred &gt; 0.5).astype(int)\n    acc = accuracy_score(y_train, y_train_pred_classes)\n    train_accuracies.append(acc)\n    if (epoch+1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Training Accuracy: {acc*100:.2f}%\")\n</code></pre>"},{"location":"pages/mlp/exercise2/#test-evaluation","title":"Test Evaluation","text":"<p>After training, we evaluate the model on the test set to measure its generalization performance.</p> <pre><code>y_test_pred = mlp.forward(X_test)\ny_test_pred_classes = (y_test_pred &gt; 0.5).astype(int)\naccuracy = accuracy_score(y_test, y_test_pred_classes)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n</code></pre> <p>Test Accuracy: 82.50%</p>"},{"location":"pages/mlp/exercise2/#training-accuracy-plot","title":"Training Accuracy Plot","text":"<p>The following plot shows how the training accuracy evolves over the epochs, helping to visualize convergence and possible overfitting.</p> <pre><code>plt.figure(figsize=(8,4))\nplt.plot(train_accuracies, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Training Accuracy')\nplt.title('Training Accuracy Over Epochs')\nplt.grid(True, alpha=0.3)\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'mlp','exercise2_accuracy.png'))\nplt.close()\n</code></pre> <p></p>"},{"location":"pages/mlp/exercise2/#decision-boundary","title":"Decision Boundary","text":"<p>Finally, we visualize the decision boundary learned by the MLP. This helps to understand how well the model separates the two classes in the feature space.</p> <pre><code>if input_size == 2:\n    plot_decision_boundary(mlp, X_test, y_test, 'exercise2_decision_boundary.png')\n</code></pre> <p></p>"},{"location":"pages/mlp/exercise3and4/","title":"Multi-Class Classification with Synthetic Data and Reusable MLP","text":"<p>This notebook demonstrates how to use a Multi-Layer Perceptron (MLP) to solve a multi-class classification problem. The process includes generating synthetic data, visualizing it, defining and training the model, evaluating performance, and visualizing results for two different MLP architectures.</p>"},{"location":"pages/mlp/exercise3and4/#data-generation","title":"Data Generation","text":"<p>We generate synthetic data for three classes, each with different cluster structures. The data is one-hot encoded and split into training and test sets.</p> <pre><code>def generate_multiclass_data():\n    \"\"\"Gera dados para os Exerc\u00edcios 3 e 4 (classifica\u00e7\u00e3o multi-classe).\"\"\"\n    n_samples = 1500\n    n_features = 4\n    random_state = 42\n\n    # Classe 0: 2 clusters\n    X0, y0 = make_classification(n_samples=int(n_samples/3), n_features=n_features, n_informative=n_features,\n                                 n_redundant=0, n_clusters_per_class=2, flip_y=0.01,\n                                 class_sep=1.0, random_state=random_state)\n    y0[:] = 0\n\n    # Classe 1: 3 clusters\n    X1, y1 = make_classification(n_samples=int(n_samples/3), n_features=n_features, n_informative=n_features,\n                                 n_redundant=0, n_clusters_per_class=3, flip_y=0.01,\n                                 class_sep=1.0, random_state=random_state + 1)\n    y1[:] = 1\n\n    # Classe 2: 4 clusters\n    X2, y2 = make_classification(n_samples=int(n_samples/3), n_features=n_features, n_informative=n_features,\n                                 n_redundant=0, n_clusters_per_class=4, flip_y=0.01,\n                                 class_sep=1.0, random_state=random_state + 2)\n    y2[:] = 2\n\n    # Combina e embaralha os dados\n    X = np.vstack((X0, X1, X2))\n    y = np.hstack((y0, y1, y2))\n\n    shuffle_indices = np.random.permutation(n_samples)\n    X = X[shuffle_indices]\n    y = y[shuffle_indices]\n\n    # Converte y para one-hot encoding\n    y_one_hot = np.eye(3)[y]\n    X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=random_state)\n\n    # Armazena os r\u00f3tulos originais para avalia\u00e7\u00e3o de acur\u00e1cia\n    y_test_labels = np.argmax(y_test, axis=1)\n\n    return X_train, X_test, y_train, y_test, y_test_labels\n\n\nX_train, X_test, y_train, y_test, y_test_labels = generate_multiclass_data()\n</code></pre>"},{"location":"pages/mlp/exercise3and4/#data-visualization-pca","title":"Data Visualization (PCA)","text":"<p>To visualize high-dimensional data, we use PCA to project it into 2D. This helps us see the class separation and structure before training.</p> <pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train)\nplt.scatter(X_train_2d[:,0], X_train_2d[:,1], c=np.argmax(y_train, axis=1), cmap='viridis', alpha=0.6)\nplt.title('Training Data (PCA) - Exercise 3/4')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.grid(True, alpha=0.3)\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'mlp','exercise3_data_pca.png'))\nplt.close()\n</code></pre> <p></p>"},{"location":"pages/mlp/exercise3and4/#exercise-3-mlp-with-1-hidden-layer","title":"Exercise 3: MLP with 1 Hidden Layer","text":"<p>In this part, we define and train an MLP with a single hidden layer to classify the data into three classes.</p>"},{"location":"pages/mlp/exercise3and4/#model-initialization","title":"Model Initialization","text":"<p>We set up the MLP architecture for multi-class classification with one hidden layer.</p> <pre><code>input_size = X_train.shape[1]\nhidden_sizes = [64]\noutput_size = 3\nlearning_rate = 0.05\nmlp = MLP(input_size, hidden_sizes, output_size, learning_rate, task='multiclass')\n</code></pre>"},{"location":"pages/mlp/exercise3and4/#training-and-accuracy-per-epoch","title":"Training and Accuracy per Epoch","text":"<p>The model is trained for 2000 epochs. We track the training accuracy at each epoch to monitor learning progress.</p> <pre><code>train_accuracies = []\nepochs = 2000\nfor epoch in range(epochs):\n    mlp.train(X_train, y_train, 1)\n    y_train_pred = mlp.forward(X_train)\n    y_train_pred_labels = np.argmax(y_train_pred, axis=1)\n    y_train_labels = np.argmax(y_train, axis=1)\n    acc = accuracy_score(y_train_labels, y_train_pred_labels)\n    train_accuracies.append(acc)\n    if (epoch+1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Training Accuracy: {acc*100:.2f}%\")\n</code></pre>"},{"location":"pages/mlp/exercise3and4/#test-evaluation","title":"Test Evaluation","text":"<p>After training, we evaluate the model on the test set to see how well it generalizes to unseen data.</p> <pre><code>y_test_pred = mlp.forward(X_test)\ny_test_pred_labels = np.argmax(y_test_pred, axis=1)\naccuracy = accuracy_score(y_test_labels, y_test_pred_labels)\nprint(f\"Test Accuracy (Exercise 3): {accuracy * 100:.2f}%\")\n</code></pre> <p>Test Accuracy (Exercise 3): 62.00%</p>"},{"location":"pages/mlp/exercise3and4/#training-accuracy-plot","title":"Training Accuracy Plot","text":"<p>The plot below shows the evolution of training accuracy over epochs, which helps to visualize the learning process and convergence.</p> <pre><code>plt.figure(figsize=(8,4))\nplt.plot(train_accuracies, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Training Accuracy')\nplt.title('Training Accuracy Over Epochs')\nplt.grid(True, alpha=0.3)\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'mlp','exercise3_accuracy.png'))\nplt.close()\n</code></pre> <p></p>"},{"location":"pages/mlp/exercise3and4/#decision-boundary-pca","title":"Decision Boundary (PCA)","text":"<p>We visualize the decision boundaries learned by the MLP in the PCA-reduced space, showing how the model separates the classes.</p> <pre><code>plot_decision_boundary_pca(mlp, X_test, y_test, pca, 'exercise3_decision_boundary_pca.png')\n</code></pre> <p></p>"},{"location":"pages/mlp/exercise3and4/#exercise-4-mlp-with-2-hidden-layers","title":"Exercise 4: MLP with 2 Hidden Layers","text":"<p>Now, we repeat the process with an MLP that has two hidden layers, which can capture more complex patterns in the data.</p>"},{"location":"pages/mlp/exercise3and4/#model-initialization_1","title":"Model Initialization","text":"<p>We set up the MLP architecture for multi-class classification with two hidden layers.</p> <pre><code>input_size = X_train.shape[1]\nhidden_sizes = [64, 32]\noutput_size = 3\nlearning_rate = 0.05\nmlp = MLP(input_size, hidden_sizes, output_size, learning_rate, task='multiclass')\n</code></pre>"},{"location":"pages/mlp/exercise3and4/#training-and-accuracy-per-epoch_1","title":"Training and Accuracy per Epoch","text":"<p>Again, we train for 2000 epochs and track the training accuracy.</p> <pre><code>train_accuracies = []\nepochs = 2000\nfor epoch in range(epochs):\n    mlp.train(X_train, y_train, 1)\n    y_train_pred = mlp.forward(X_train)\n    y_train_pred_labels = np.argmax(y_train_pred, axis=1)\n    y_train_labels = np.argmax(y_train, axis=1)\n    acc = accuracy_score(y_train_labels, y_train_pred_labels)\n    train_accuracies.append(acc)\n    if (epoch+1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Training Accuracy: {acc*100:.2f}%\")\n</code></pre>"},{"location":"pages/mlp/exercise3and4/#test-evaluation_1","title":"Test Evaluation","text":"<p>We evaluate the two-hidden-layer MLP on the test set to compare its performance with the previous model.</p> <pre><code>y_test_pred = mlp.forward(X_test)\ny_test_pred_labels = np.argmax(y_test_pred, axis=1)\naccuracy = accuracy_score(y_test_labels, y_test_pred_labels)\nprint(f\"Test Accuracy (Exercise 4): {accuracy * 100:.2f}%\")\n</code></pre> <p>Test Accuracy (Exercise 4): 68.67%</p>"},{"location":"pages/mlp/exercise3and4/#training-accuracy-plot_1","title":"Training Accuracy Plot","text":"<p>The following plot shows the training accuracy over epochs for the two-hidden-layer MLP.</p> <pre><code>plt.figure(figsize=(8,4))\nplt.plot(train_accuracies, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Training Accuracy')\nplt.title('Training Accuracy Over Epochs')\nplt.grid(True, alpha=0.3)\nplt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'mlp','exercise4_accuracy.png'))\nplt.close()\n</code></pre> <p></p>"},{"location":"pages/mlp/exercise3and4/#decision-boundary-pca_1","title":"Decision Boundary (PCA)","text":"<p>Finally, we visualize the decision boundaries for the two-hidden-layer MLP in the PCA-reduced space.</p> <pre><code>plot_decision_boundary_pca(mlp, X_test, y_test, pca, 'exercise4_decision_boundary_pca.png')\n</code></pre> <p></p>"},{"location":"pages/mlp/mlp/","title":"MLP Implementation","text":""},{"location":"pages/mlp/mlp/#mlp-implementation","title":"MLP Implementation","text":"<p>Here is a simple implementation of a Multi-Layer Perceptron (MLP) in Python using NumPy. This implementation includes forward and backward propagation, as well as parameter updates using gradient descent. The MLP can be configured for binary or multi-class classification tasks.</p> <pre><code>import numpy as np\n\nclass MLP:\n    def __init__(self, input_size, hidden_sizes, output_size, learning_rate=0.01, task='binary'):\n        self.lr = learning_rate\n        self.task = task\n        self.hidden_sizes = hidden_sizes\n        self.output_size = output_size\n        self.layers = []  # Lista para armazenar pesos e vieses de cada camada\n\n        # Cria\u00e7\u00e3o das camadas da rede\n        layer_sizes = [input_size] + hidden_sizes + [output_size]\n\n        for i in range(len(layer_sizes) - 1):\n            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0/layer_sizes[i])\n            b = np.zeros((1, layer_sizes[i+1]))\n            self.layers.append({'W': W, 'b': b, 'a': None, 'z': None}) # 'a' e 'z' armazenam ativa\u00e7\u00f5es para o backprop\n\n    # Fun\u00e7\u00f5es de ativa\u00e7\u00e3o\n    def _tanh(self, z):\n        return np.tanh(z)\n\n    def _tanh_derivative(self, a):\n        return 1 - np.power(a, 2)\n\n    def _sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def _softmax(self, z):\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    # Fun\u00e7\u00f5es de perda\n    def _binary_cross_entropy(self, y_true, y_pred):\n        m = y_true.shape[0]\n        loss = -(1/m) * np.sum(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9))\n        return loss\n\n    def _categorical_cross_entropy(self, y_true, y_pred):\n        m = y_true.shape[0]\n        loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n        return loss\n\n    def forward(self, X):\n        activations = [X]\n        # Forward pass para todas as camadas ocultas\n        for i in range(len(self.hidden_sizes)):\n            z = np.dot(activations[i], self.layers[i]['W']) + self.layers[i]['b']\n            a = self._tanh(z)\n            self.layers[i]['z'] = z\n            self.layers[i]['a'] = a\n            activations.append(a)\n\n        # Forward pass para a camada de sa\u00edda\n        z_out = np.dot(activations[-1], self.layers[-1]['W']) + self.layers[-1]['b']\n        self.layers[-1]['z'] = z_out\n\n        if self.task == 'binary':\n            a_out = self._sigmoid(z_out)\n        elif self.task == 'multiclass':\n            a_out = self._softmax(z_out)\n        self.layers[-1]['a'] = a_out\n\n        return a_out\n\n    def backward(self, X, y_true, y_pred):\n        m = X.shape[0]\n        grads = {}\n\n        # Gradiente para a camada de sa\u00edda (a \u00faltima camada na lista)\n        dz = (y_pred - y_true) / m\n        grads[len(self.layers)-1] = {\n            'dW': np.dot(self.layers[-2]['a'].T if len(self.layers) &gt; 1 else X.T, dz),\n            'db': np.sum(dz, axis=0, keepdims=True)\n        }\n\n        # Backward pass para as camadas ocultas (loop de tr\u00e1s para frente)\n        for i in range(len(self.layers) - 2, -1, -1):\n            da = np.dot(dz, self.layers[i+1]['W'].T)\n            dz = da * self._tanh_derivative(self.layers[i]['a'])\n\n            # O input para a primeira camada oculta \u00e9 X\n            input_a = self.layers[i-1]['a'] if i &gt; 0 else X\n\n            grads[i] = {\n                'dW': np.dot(input_a.T, dz),\n                'db': np.sum(dz, axis=0, keepdims=True)\n            }\n\n        return grads\n\n    def update_parameters(self, grads):\n        for i in range(len(self.layers)):\n            self.layers[i]['W'] -= self.lr * grads[i]['dW']\n            self.layers[i]['b'] -= self.lr * grads[i]['db']\n\n    def train(self, X_train, y_train, epochs):\n        losses = []\n        for epoch in range(epochs):\n            y_pred = self.forward(X_train)\n\n            if self.task == 'binary':\n                loss = self._binary_cross_entropy(y_train, y_pred)\n            elif self.task == 'multiclass':\n                loss = self._categorical_cross_entropy(y_train, y_pred)\n\n            grads = self.backward(X_train, y_train, y_pred)\n            self.update_parameters(grads)\n            losses.append(loss)\n            if (epoch + 1) % 100 == 0:\n                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n        return losses\n</code></pre>"},{"location":"pages/perceptron/exercise1/","title":"Perceptron and Linearly Separable Data","text":""},{"location":"pages/perceptron/exercise1/#data-generation-and-visualization","title":"Data generation and visualization","text":"<pre><code>PARAMS_EX1 = {\n        0 : {\n            MEAN : [1.5, 1.5], \n            STD : [[0.5, 0], [0, 0.5]]\n        },\n        1 : {\n            MEAN : [5, 5], \n            STD : [[0.5, 0], [0, 0.5]]\n        }\n    }\n\ndef generate_data(params, n_samples = 1000):\n    X = []\n    y = []\n\n    cls_A = np.random.multivariate_normal(params[0][MEAN], params[0][STD], n_samples)\n    cls_B = np.random.multivariate_normal(params[1][MEAN], params[1][STD], n_samples)\n    labels_A = np.zeros(n_samples)\n    labels_B = np.ones(n_samples)\n\n    X = np.vstack((cls_A, cls_B))\n    y = np.hstack((labels_A, labels_B))\n\n    return X, y\n\ndef save_plot_data(X, y, params, img_name):\n    colors = ['red', 'blue']\n\n    plt.figure(figsize=(8, 6))\n    for cls in params.keys():\n        plt.scatter(X[y == cls, 0], X[y == cls, 1], c=colors[cls], label=f\"Class {cls}\", alpha=0.6)\n\n    plt.xlabel(\"Label 1\")\n    plt.ylabel(\"Label 2\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'perceptron',img_name))  \n</code></pre> <p>This code generates a synthetic 2D dataset for a binary classification task. We'll create two distinct classes, each with 1,000 data points, using multivariate normal distributions. This is a common practice in machine learning to create controlled datasets for testing algorithms.</p> <p>The key parameters for each class are its mean (the center of the data cluster) and its covariance matrix (which defines the spread and orientation of the data points). In this case, the low variance of 0.5 along each dimension and a large distance between the class means ([1.5, 1.5] vs. [5, 5]) ensure that the two classes form tight, well-separated clusters. This configuration results in a dataset that is linearly separable\u2014meaning a single straight line can perfectly divide the two classes.</p> <p></p>"},{"location":"pages/perceptron/exercise1/#perceptron-implementation","title":"Perceptron Implementation","text":"<pre><code>class Perceptron:\n    def __init__(self, tag, w=np.zeros(2), b=1, learning_rate=0.01, max_epochs=100):\n        self.tag = tag\n        self.max_epochs = max_epochs\n        self.learning_rate = learning_rate\n        self.w = w\n        self.b = b\n        self.training_accuracies = []\n        self.predict_function = lambda x: np.dot(self.w, x) + self.b\n        self.activation_function = lambda z: 1 if z &gt;= 0 else 0 \n\n    def classify(self, x):\n        z = self.predict_function(x)\n        return self.activation_function(z)\n\n    def train_epoch(self, cls, labels):\n        weights_updated = False\n        errors_count = 0\n\n        for x, label in zip(cls, labels):\n            x = np.array(x)\n            result = self.classify(x)\n\n            error = label - result\n\n            if error != 0:\n                self.w += self.learning_rate * error * x\n                self.b += self.learning_rate * error\n                weights_updated = True\n                errors_count += 1\n        print(f\"Erros na \u00e9poca: {errors_count}\")\n        return weights_updated\n\n    def train(self, cls, labels):\n        weights_updated = True\n        current_epoch = 1\n        accuracy_list = [self.evaluate(cls, labels)]\n        weights_history = [self.w.copy()]\n        bias_history = [self.b]\n        X = np.array(cls)\n        y = np.array(labels)\n        while weights_updated and current_epoch &lt;= self.max_epochs:\n            weights_updated = self.train_epoch(cls, labels)\n            acc = self.evaluate(cls, labels)\n            accuracy_list.append(acc)\n            weights_history.append(self.w.copy())\n            bias_history.append(self.b)\n            current_epoch += 1\n        print(f\"Training finished at epoch {current_epoch-1}\")\n        accuracy_list = accuracy_list[:-1]\n        weights_history = weights_history[:-1]\n        bias_history = bias_history[:-1]\n        self.plot_accuracy(accuracy_list)\n        self.plot_decision_boundary(X, y)\n        self.animate_training(X, y, accuracy_list, weights_history, bias_history)\n\n    def evaluate(self, cls, labels, verbose=True):\n        correct = 0\n        for x, label in zip(cls, labels):\n            result = self.classify(x)\n            if result == label:\n                correct += 1\n        accuracy = correct / len(labels)\n        if verbose:\n            print(f\"Acur\u00e1cia: {accuracy*100:.2f}%\")\n        return accuracy\n\n    def plot_decision_boundary(self, X, y):\n        plt.figure(figsize=(8,6))\n        for label, marker, color in zip([0,1], ['o','s'], ['red','blue']):\n            plt.scatter(X[y==label,0], X[y==label,1], marker=marker, color=color, label=f\"Class {label}\", alpha=0.6)\n        x_vals = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 100)\n        if self.w[1] != 0:\n            y_vals = -(self.w[0]*x_vals + self.b)/self.w[1]\n            plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n        plt.xlabel('x1')\n        plt.ylabel('x2')\n        plt.legend()\n        plt.title('Decision Boundary and Data')\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH, 'perceptron', f'decision_boundary_{self.tag}.png'))\n\n    def plot_accuracy(self, accuracy_list):\n        plt.figure(figsize=(8,4))\n        plt.plot(accuracy_list, marker='o')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.title('Training Accuracy Over Epochs')\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH, 'perceptron', f'accuracy_{self.tag}.png'))\n\n    def animate_training(self, X, y, accuracy_list, weights_history, bias_history):\n        fig, ax = plt.subplots(figsize=(8,6))\n        xlim = (X[:,0].min()-1, X[:,0].max()+1)\n        ylim = (X[:,1].min()-1, X[:,1].max()+1)\n        def update(epoch):\n            ax.clear()\n            for label, marker, color in zip([0,1], ['o','s'], ['red','blue']):\n                ax.scatter(X[y==label,0], X[y==label,1], marker=marker, color=color, label=f\"Class {label}\", alpha=0.6)\n            w = weights_history[epoch]\n            b = bias_history[epoch]\n            if w[1] != 0:\n                x_vals = np.linspace(xlim[0], xlim[1], 100)\n                y_vals = -(w[0]*x_vals + b)/w[1]\n                ax.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n            misclassified = []\n            for i, (x_point, label_point) in enumerate(zip(X, y)):\n                pred = 1 if np.dot(w, x_point) + b &gt;= 0 else 0\n                if pred != label_point:\n                    misclassified.append(i)\n            if misclassified:\n                ax.scatter(X[misclassified,0], X[misclassified,1], c='yellow', marker='x', s=100, label='Misclassified')\n            ax.set_title(f'Epoch {epoch+1} | Accuracy: {accuracy_list[epoch]*100:.2f}%')\n            ax.set_xlabel('x1')\n            ax.set_ylabel('x2')\n            ax.set_xlim(xlim)\n            ax.set_ylim(ylim)\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        anim = FuncAnimation(fig, update, frames=len(weights_history), interval=1500)\n        anim_path = os.path.join(GIFS_OUTPUTS_FILE_PATH, 'perceptron', f'animate_training_{self.tag}.gif')\n        anim.save(anim_path, writer=PillowWriter(fps=2))\n        plt.close()\n</code></pre>"},{"location":"pages/perceptron/exercise1/#accuracy-during-training","title":"Accuracy during training","text":""},{"location":"pages/perceptron/exercise1/#decision-boundary","title":"Decision Boundary","text":"<p>On the linearly separable dataset, the perceptron achieved 100.00% accuracy, demonstrating perfect classification. The model converged very quickly, finishing training in just 15 epochs with final weights of w = [0.02128936, 0.0124718] and a bias of b = -0.11.</p> <p>The rapid convergence to a perfect solution is a direct consequence of the Perceptron Convergence Theorem. This theorem guarantees that if a dataset is linearly separable\u2014meaning a single straight line can perfectly divide the classes\u2014the perceptron algorithm is guaranteed to find that separating line in a finite number of steps.</p> <p>In our specific case, the data was not only linearly separable but also had:</p> <p>Large Distance Between Means: The clusters were far apart.</p> <p>Low Variance: The data points within each cluster were tightly packed.</p> <p>These characteristics make the classification task trivial for the perceptron. With each weight update, the model's decision boundary moves closer to the ideal separation line. Since there is no overlap, the algorithm efficiently finds a valid solution without having to \"oscillate\" or make compromises to correct misclassified points from different classes. The low number of epochs confirms that a clear and simple solution was found very quickly.</p>"},{"location":"pages/perceptron/exercise1/#final-code","title":"Final Code:","text":"<p>The complete code for the perceptron implementation and training process can be found below:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nfrom src.utils import *\nimport os\n\nclass Perceptron:\n    def __init__(self, tag, w=np.zeros(2), b=1, learning_rate=0.01, max_epochs=100):\n        self.tag = tag\n        self.max_epochs = max_epochs\n        self.learning_rate = learning_rate\n        self.w = w\n        self.b = b\n        self.training_accuracies = []\n        self.predict_function = lambda x: np.dot(self.w, x) + self.b\n        self.activation_function = lambda z: 1 if z &gt;= 0 else 0 \n\n    def classify(self, x):\n        z = self.predict_function(x)\n        return self.activation_function(z)\n\n    def train_epoch(self, cls, labels):\n        weights_updated = False\n        errors_count = 0\n\n        for x, label in zip(cls, labels):\n            x = np.array(x)\n            result = self.classify(x)\n\n            error = label - result\n\n            if error != 0:\n                self.w += self.learning_rate * error * x\n                self.b += self.learning_rate * error\n                weights_updated = True\n                errors_count += 1\n        print(f\"Erros na \u00e9poca: {errors_count}\")\n        return weights_updated\n\n    def train(self, cls, labels):\n        weights_updated = True\n        current_epoch = 1\n        accuracy_list = [self.evaluate(cls, labels)]\n        weights_history = [self.w.copy()]\n        bias_history = [self.b]\n        X = np.array(cls)\n        y = np.array(labels)\n        while weights_updated and current_epoch &lt;= self.max_epochs:\n            weights_updated = self.train_epoch(cls, labels)\n            acc = self.evaluate(cls, labels)\n            accuracy_list.append(acc)\n            weights_history.append(self.w.copy())\n            bias_history.append(self.b)\n            current_epoch += 1\n        print(f\"Training finished at epoch {current_epoch-1}\")\n        accuracy_list = accuracy_list[:-1]\n        weights_history = weights_history[:-1]\n        bias_history = bias_history[:-1]\n        self.plot_accuracy(accuracy_list)\n        self.plot_decision_boundary(X, y)\n        self.animate_training(X, y, accuracy_list, weights_history, bias_history)\n\n    def evaluate(self, cls, labels, verbose=True):\n        correct = 0\n        for x, label in zip(cls, labels):\n            result = self.classify(x)\n            if result == label:\n                correct += 1\n        accuracy = correct / len(labels)\n        if verbose:\n            print(f\"Acur\u00e1cia: {accuracy*100:.2f}%\")\n        return accuracy\n\n    def plot_decision_boundary(self, X, y):\n        plt.figure(figsize=(8,6))\n        for label, marker, color in zip([0,1], ['o','s'], ['red','blue']):\n            plt.scatter(X[y==label,0], X[y==label,1], marker=marker, color=color, label=f\"Class {label}\", alpha=0.6)\n        x_vals = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 100)\n        if self.w[1] != 0:\n            y_vals = -(self.w[0]*x_vals + self.b)/self.w[1]\n            plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n        plt.xlabel('x1')\n        plt.ylabel('x2')\n        plt.legend()\n        plt.title('Decision Boundary and Data')\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH, 'perceptron', f'decision_boundary_{self.tag}.png'))\n\n    def plot_accuracy(self, accuracy_list):\n        plt.figure(figsize=(8,4))\n        plt.plot(accuracy_list, marker='o')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.title('Training Accuracy Over Epochs')\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH, 'perceptron', f'accuracy_{self.tag}.png'))\n\n    def animate_training(self, X, y, accuracy_list, weights_history, bias_history):\n        fig, ax = plt.subplots(figsize=(8,6))\n        xlim = (X[:,0].min()-1, X[:,0].max()+1)\n        ylim = (X[:,1].min()-1, X[:,1].max()+1)\n        def update(epoch):\n            ax.clear()\n            for label, marker, color in zip([0,1], ['o','s'], ['red','blue']):\n                ax.scatter(X[y==label,0], X[y==label,1], marker=marker, color=color, label=f\"Class {label}\", alpha=0.6)\n            w = weights_history[epoch]\n            b = bias_history[epoch]\n            if w[1] != 0:\n                x_vals = np.linspace(xlim[0], xlim[1], 100)\n                y_vals = -(w[0]*x_vals + b)/w[1]\n                ax.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n            misclassified = []\n            for i, (x_point, label_point) in enumerate(zip(X, y)):\n                pred = 1 if np.dot(w, x_point) + b &gt;= 0 else 0\n                if pred != label_point:\n                    misclassified.append(i)\n            if misclassified:\n                ax.scatter(X[misclassified,0], X[misclassified,1], c='yellow', marker='x', s=100, label='Misclassified')\n            ax.set_title(f'Epoch {epoch+1} | Accuracy: {accuracy_list[epoch]*100:.2f}%')\n            ax.set_xlabel('x1')\n            ax.set_ylabel('x2')\n            ax.set_xlim(xlim)\n            ax.set_ylim(ylim)\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        anim = FuncAnimation(fig, update, frames=len(weights_history), interval=1500)\n        anim_path = os.path.join(GIFS_OUTPUTS_FILE_PATH, 'perceptron', f'animate_training_{self.tag}.gif')\n        anim.save(anim_path, writer=PillowWriter(fps=2))\n        plt.close()\n\nimport numpy as np\nfrom src.utils import *\nimport matplotlib.pyplot as plt\nfrom src.perceptron.perceptron import Perceptron\n\nPARAMS_EX1 = {\n        0 : {\n            MEAN : [1.5, 1.5], \n            STD : [[0.5, 0], [0, 0.5]]\n        },\n        1 : {\n            MEAN : [5, 5], \n            STD : [[0.5, 0], [0, 0.5]]\n        }\n    }\n\ndef generate_data(params, n_samples = 1000):\n    X = []\n    y = []\n\n    cls_A = np.random.multivariate_normal(params[0][MEAN], params[0][STD], n_samples)\n    cls_B = np.random.multivariate_normal(params[1][MEAN], params[1][STD], n_samples)\n    labels_A = np.zeros(n_samples)\n    labels_B = np.ones(n_samples)\n\n    X = np.vstack((cls_A, cls_B))\n    y = np.hstack((labels_A, labels_B))\n\n    return X, y\n\ndef save_plot_data(X, y, params, img_name):\n    colors = ['red', 'blue']\n\n    plt.figure(figsize=(8, 6))\n    for cls in params.keys():\n        plt.scatter(X[y == cls, 0], X[y == cls, 1], c=colors[cls], label=f\"Class {cls}\", alpha=0.6)\n\n    plt.xlabel(\"Label 1\")\n    plt.ylabel(\"Label 2\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'perceptron',img_name))  \n\ndef exercise1():\n    X, y  = generate_data(params=PARAMS_EX1, n_samples=1000)\n    save_plot_data(X, y, PARAMS_EX1,'exercise1_1.png')\n    perceptron = Perceptron('ex1')\n    perceptron.train(X, y)\n    print(f'Final training weights: {perceptron.w}, and Bias: {perceptron.b}')\n\ndef main():\n    exercise1()\n\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    main()\n</code></pre>"},{"location":"pages/perceptron/exercise2/","title":"Perceptron and Not Fully Linearly Separable Data","text":""},{"location":"pages/perceptron/exercise2/#data-generation-and-visualization","title":"Data generation and visualization","text":"<pre><code>PARAMS_EX2 = {\n        0 : {\n            MEAN : [3, 3], \n            STD : [[1.5, 0], [0, 1.5]]\n        },\n        1 : {\n            MEAN : [4, 4], \n            STD : [[1.5, 0], [0, 1.5]]\n        }\n    }\n\ndef generate_data(params, n_samples = 1000):\n    X = []\n    y = []\n\n    cls_A = np.random.multivariate_normal(params[0][MEAN], params[0][STD], n_samples)\n    cls_B = np.random.multivariate_normal(params[1][MEAN], params[1][STD], n_samples)\n    labels_A = np.zeros(n_samples)\n    labels_B = np.ones(n_samples)\n\n    X = np.vstack((cls_A, cls_B))\n    y = np.hstack((labels_A, labels_B))\n\n    return X, y\n\ndef save_plot_data(X, y, params, img_name):\n    colors = ['red', 'blue']\n\n    plt.figure(figsize=(8, 6))\n    for cls in params.keys():\n        plt.scatter(X[y == cls, 0], X[y == cls, 1], c=colors[cls], label=f\"Class {cls}\", alpha=0.6)\n\n    plt.xlabel(\"Label 1\")\n    plt.ylabel(\"Label 2\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'perceptron',img_name))  \n</code></pre> <p>The data consists of two distinct classes, each with 1,000 data points generated from a multivariate normal distribution.</p> <p>The parameters used for this task are designed to create a problem with significant overlap:</p> <p>Higher Variance: A variance of 1.5 causes the data points to be more dispersed.</p> <p>Closer Means: The means of [3, 3] and [4, 4] are close together, bringing the two clusters into close proximity.</p> <p>The combination of higher variance and closer means results in a dataset where the classes mix, making it impossible to separate them with a single straight line. The scatter plot below visually confirms this high degree of overlap.</p> <p></p>"},{"location":"pages/perceptron/exercise2/#perceptron-implementation","title":"Perceptron Implementation","text":"<pre><code>class Perceptron:\n    def __init__(self, tag, w=np.zeros(2), b=1, learning_rate=0.01, max_epochs=100):\n        self.tag = tag\n        self.max_epochs = max_epochs\n        self.learning_rate = learning_rate\n        self.w = w\n        self.b = b\n        self.training_accuracies = []\n        self.predict_function = lambda x: np.dot(self.w, x) + self.b\n        self.activation_function = lambda z: 1 if z &gt;= 0 else 0 \n\n    def classify(self, x):\n        z = self.predict_function(x)\n        return self.activation_function(z)\n\n    def train_epoch(self, cls, labels):\n        weights_updated = False\n        errors_count = 0\n\n        for x, label in zip(cls, labels):\n            x = np.array(x)\n            result = self.classify(x)\n\n            error = label - result\n\n            if error != 0:\n                self.w += self.learning_rate * error * x\n                self.b += self.learning_rate * error\n                weights_updated = True\n                errors_count += 1\n        print(f\"Erros na \u00e9poca: {errors_count}\")\n        return weights_updated\n\n    def train(self, cls, labels):\n        weights_updated = True\n        current_epoch = 1\n        accuracy_list = [self.evaluate(cls, labels)]\n        weights_history = [self.w.copy()]\n        bias_history = [self.b]\n        X = np.array(cls)\n        y = np.array(labels)\n        while weights_updated and current_epoch &lt;= self.max_epochs:\n            weights_updated = self.train_epoch(cls, labels)\n            acc = self.evaluate(cls, labels)\n            accuracy_list.append(acc)\n            weights_history.append(self.w.copy())\n            bias_history.append(self.b)\n            current_epoch += 1\n        print(f\"Training finished at epoch {current_epoch-1}\")\n        accuracy_list = accuracy_list[:-1]\n        weights_history = weights_history[:-1]\n        bias_history = bias_history[:-1]\n        self.plot_accuracy(accuracy_list)\n        self.plot_decision_boundary(X, y)\n        self.animate_training(X, y, accuracy_list, weights_history, bias_history)\n\n    def evaluate(self, cls, labels, verbose=True):\n        correct = 0\n        for x, label in zip(cls, labels):\n            result = self.classify(x)\n            if result == label:\n                correct += 1\n        accuracy = correct / len(labels)\n        if verbose:\n            print(f\"Acur\u00e1cia: {accuracy*100:.2f}%\")\n        return accuracy\n\n    def plot_decision_boundary(self, X, y):\n        plt.figure(figsize=(8,6))\n        for label, marker, color in zip([0,1], ['o','s'], ['red','blue']):\n            plt.scatter(X[y==label,0], X[y==label,1], marker=marker, color=color, label=f\"Class {label}\", alpha=0.6)\n        x_vals = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 100)\n        if self.w[1] != 0:\n            y_vals = -(self.w[0]*x_vals + self.b)/self.w[1]\n            plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n        plt.xlabel('x1')\n        plt.ylabel('x2')\n        plt.legend()\n        plt.title('Decision Boundary and Data')\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH, 'perceptron', f'decision_boundary_{self.tag}.png'))\n\n    def plot_accuracy(self, accuracy_list):\n        plt.figure(figsize=(8,4))\n        plt.plot(accuracy_list, marker='o')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.title('Training Accuracy Over Epochs')\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH, 'perceptron', f'accuracy_{self.tag}.png'))\n\n    def animate_training(self, X, y, accuracy_list, weights_history, bias_history):\n        fig, ax = plt.subplots(figsize=(8,6))\n        xlim = (X[:,0].min()-1, X[:,0].max()+1)\n        ylim = (X[:,1].min()-1, X[:,1].max()+1)\n        def update(epoch):\n            ax.clear()\n            for label, marker, color in zip([0,1], ['o','s'], ['red','blue']):\n                ax.scatter(X[y==label,0], X[y==label,1], marker=marker, color=color, label=f\"Class {label}\", alpha=0.6)\n            w = weights_history[epoch]\n            b = bias_history[epoch]\n            if w[1] != 0:\n                x_vals = np.linspace(xlim[0], xlim[1], 100)\n                y_vals = -(w[0]*x_vals + b)/w[1]\n                ax.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n            misclassified = []\n            for i, (x_point, label_point) in enumerate(zip(X, y)):\n                pred = 1 if np.dot(w, x_point) + b &gt;= 0 else 0\n                if pred != label_point:\n                    misclassified.append(i)\n            if misclassified:\n                ax.scatter(X[misclassified,0], X[misclassified,1], c='yellow', marker='x', s=100, label='Misclassified')\n            ax.set_title(f'Epoch {epoch+1} | Accuracy: {accuracy_list[epoch]*100:.2f}%')\n            ax.set_xlabel('x1')\n            ax.set_ylabel('x2')\n            ax.set_xlim(xlim)\n            ax.set_ylim(ylim)\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        anim = FuncAnimation(fig, update, frames=len(weights_history), interval=1500)\n        anim_path = os.path.join(GIFS_OUTPUTS_FILE_PATH, 'perceptron', f'animate_training_{self.tag}.gif')\n        anim.save(anim_path, writer=PillowWriter(fps=2))\n        plt.close()\n</code></pre>"},{"location":"pages/perceptron/exercise2/#accuracy-during-training","title":"Accuracy during training","text":""},{"location":"pages/perceptron/exercise2/#decision-boundary","title":"Decision boundary","text":"<p>On the non linearly separable dataset, the perceptron achieved maximum of 50.05% accuracy, demonstrating difficulty in learning the decision boundary. The model struggled to converge, with weights and bias oscillating without settling. The final decision boundary failed to effectively separate the classes, highlighting the perceptron's limitations on complex datasets. After the maximum epochs, the weights were [0.04358138 0.02465982] and the bias -0.010000000000000753.</p> <p>With this results we can note two main points:</p> <p>The Lack of Convergence: There is no single decision line that can correctly classify all points. Each time the model corrects a misclassified point, it causes another point to become misclassified. This leads to an endless loop of weight updates and a constant oscillation in the model's accuracy. The final non-zero number of errors in the last epoch proves that a perfect solution was not found.</p> <p>Model Limitations: This experiment highlights a core limitation of the single-layer perceptron. For problems with overlapping data, a more complex model, such as a multi-layer neural network with non-linear activation functions, would be required to learn the intricate decision boundaries necessary for accurate classification.</p>"},{"location":"pages/perceptron/exercise2/#final-code","title":"Final Code:","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nfrom src.utils import *\nimport os\n\nclass Perceptron:\n    def __init__(self, tag, w=np.zeros(2), b=1, learning_rate=0.01, max_epochs=100):\n        self.tag = tag\n        self.max_epochs = max_epochs\n        self.learning_rate = learning_rate\n        self.w = w\n        self.b = b\n        self.training_accuracies = []\n        self.predict_function = lambda x: np.dot(self.w, x) + self.b\n        self.activation_function = lambda z: 1 if z &gt;= 0 else 0 \n\n    def classify(self, x):\n        z = self.predict_function(x)\n        return self.activation_function(z)\n\n    def train_epoch(self, cls, labels):\n        weights_updated = False\n        errors_count = 0\n\n        for x, label in zip(cls, labels):\n            x = np.array(x)\n            result = self.classify(x)\n\n            error = label - result\n\n            if error != 0:\n                self.w += self.learning_rate * error * x\n                self.b += self.learning_rate * error\n                weights_updated = True\n                errors_count += 1\n        print(f\"Erros na \u00e9poca: {errors_count}\")\n        return weights_updated\n\n    def train(self, cls, labels):\n        weights_updated = True\n        current_epoch = 1\n        accuracy_list = [self.evaluate(cls, labels)]\n        weights_history = [self.w.copy()]\n        bias_history = [self.b]\n        X = np.array(cls)\n        y = np.array(labels)\n        while weights_updated and current_epoch &lt;= self.max_epochs:\n            weights_updated = self.train_epoch(cls, labels)\n            acc = self.evaluate(cls, labels)\n            accuracy_list.append(acc)\n            weights_history.append(self.w.copy())\n            bias_history.append(self.b)\n            current_epoch += 1\n        print(f\"Training finished at epoch {current_epoch-1}\")\n        accuracy_list = accuracy_list[:-1]\n        weights_history = weights_history[:-1]\n        bias_history = bias_history[:-1]\n        self.plot_accuracy(accuracy_list)\n        self.plot_decision_boundary(X, y)\n        self.animate_training(X, y, accuracy_list, weights_history, bias_history)\n\n    def evaluate(self, cls, labels, verbose=True):\n        correct = 0\n        for x, label in zip(cls, labels):\n            result = self.classify(x)\n            if result == label:\n                correct += 1\n        accuracy = correct / len(labels)\n        if verbose:\n            print(f\"Acur\u00e1cia: {accuracy*100:.2f}%\")\n        return accuracy\n\n    def plot_decision_boundary(self, X, y):\n        plt.figure(figsize=(8,6))\n        for label, marker, color in zip([0,1], ['o','s'], ['red','blue']):\n            plt.scatter(X[y==label,0], X[y==label,1], marker=marker, color=color, label=f\"Class {label}\", alpha=0.6)\n        x_vals = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 100)\n        if self.w[1] != 0:\n            y_vals = -(self.w[0]*x_vals + self.b)/self.w[1]\n            plt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n        plt.xlabel('x1')\n        plt.ylabel('x2')\n        plt.legend()\n        plt.title('Decision Boundary and Data')\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH, 'perceptron', f'decision_boundary_{self.tag}.png'))\n\n    def plot_accuracy(self, accuracy_list):\n        plt.figure(figsize=(8,4))\n        plt.plot(accuracy_list, marker='o')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.title('Training Accuracy Over Epochs')\n        plt.grid(True, alpha=0.3)\n        plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH, 'perceptron', f'accuracy_{self.tag}.png'))\n\n    def animate_training(self, X, y, accuracy_list, weights_history, bias_history):\n        fig, ax = plt.subplots(figsize=(8,6))\n        xlim = (X[:,0].min()-1, X[:,0].max()+1)\n        ylim = (X[:,1].min()-1, X[:,1].max()+1)\n        def update(epoch):\n            ax.clear()\n            for label, marker, color in zip([0,1], ['o','s'], ['red','blue']):\n                ax.scatter(X[y==label,0], X[y==label,1], marker=marker, color=color, label=f\"Class {label}\", alpha=0.6)\n            w = weights_history[epoch]\n            b = bias_history[epoch]\n            if w[1] != 0:\n                x_vals = np.linspace(xlim[0], xlim[1], 100)\n                y_vals = -(w[0]*x_vals + b)/w[1]\n                ax.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\n            misclassified = []\n            for i, (x_point, label_point) in enumerate(zip(X, y)):\n                pred = 1 if np.dot(w, x_point) + b &gt;= 0 else 0\n                if pred != label_point:\n                    misclassified.append(i)\n            if misclassified:\n                ax.scatter(X[misclassified,0], X[misclassified,1], c='yellow', marker='x', s=100, label='Misclassified')\n            ax.set_title(f'Epoch {epoch+1} | Accuracy: {accuracy_list[epoch]*100:.2f}%')\n            ax.set_xlabel('x1')\n            ax.set_ylabel('x2')\n            ax.set_xlim(xlim)\n            ax.set_ylim(ylim)\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        anim = FuncAnimation(fig, update, frames=len(weights_history), interval=1500)\n        anim_path = os.path.join(GIFS_OUTPUTS_FILE_PATH, 'perceptron', f'animate_training_{self.tag}.gif')\n        anim.save(anim_path, writer=PillowWriter(fps=2))\n        plt.close()\n\nimport numpy as np\nfrom src.utils import *\nimport matplotlib.pyplot as plt\nfrom src.perceptron.perceptron import Perceptron\n\nPARAMS_EX2 = {\n        0 : {\n            MEAN : [3, 3], \n            STD : [[1.5, 0], [0, 1.5]]\n        },\n        1 : {\n            MEAN : [4, 4], \n            STD : [[1.5, 0], [0, 1.5]]\n        }\n    }\n\ndef generate_data(params, n_samples = 1000):\n    X = []\n    y = []\n\n    cls_A = np.random.multivariate_normal(params[0][MEAN], params[0][STD], n_samples)\n    cls_B = np.random.multivariate_normal(params[1][MEAN], params[1][STD], n_samples)\n    labels_A = np.zeros(n_samples)\n    labels_B = np.ones(n_samples)\n\n    X = np.vstack((cls_A, cls_B))\n    y = np.hstack((labels_A, labels_B))\n\n    return X, y\n\ndef save_plot_data(X, y, params, img_name):\n    colors = ['red', 'blue']\n\n    plt.figure(figsize=(8, 6))\n    for cls in params.keys():\n        plt.scatter(X[y == cls, 0], X[y == cls, 1], c=colors[cls], label=f\"Class {cls}\", alpha=0.6)\n\n    plt.xlabel(\"Label 1\")\n    plt.ylabel(\"Label 2\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig(os.path.join(IMAGES_OUTPUTS_FILE_PATH,'perceptron',img_name))  \n\ndef exercise2():\n    X2, y2 = generate_data(params=PARAMS_EX2, n_samples=1000)\n    save_plot_data(X2, y2, PARAMS_EX2, 'exercise2_1.png')\n    perceptron = Perceptron('ex2')\n    perceptron.train(X2,y2)\n    print(f'Final training weights: {perceptron.w}, and Bias: {perceptron.b}')\n\ndef main():\n    exercise2()\n\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    main()\n</code></pre>"}]}